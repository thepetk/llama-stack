# Note: You only need to set the variables you normally would with '-e' flags.
# You do not need to set them all if they will go unused.

# Enable Inference Providers
## Set any providers you want enabled to 'true'
## E.g. ENABLE_VLLM=true
## Leave all disabled providers EMPTY
## E.g. ENABLE_OPENAI=
ENABLE_VLLM=
ENABLE_GEMINI=true
ENABLE_OPENAI=
ENABLE_OLLAMA=

# vLLM Inference Settings
VLLM_URL=
VLLM_API_KEY=
# vLLM Optional Variables
VLLM_MAX_TOKENS=
VLLM_TLS_VERIFY=

# OpenAI Inference Settings
OPENAI_API_KEY=

# Gemini/Vertex AI Inference Settings
GEMINI_API_KEY=
VERTEX_AI_PROJECT=
VERTEX_AI_LOCATION=us-central1

# Ollama Inference Settings
OLLAMA_URL=

# Question Validation Safety Shield Settings (REMOVED)
## The question validation safety shield has been removed from run.yaml
# VALIDATION_PROVIDER=
# VALIDATION_MODEL_NAME=

# Other
LLAMA_STACK_LOGGING=